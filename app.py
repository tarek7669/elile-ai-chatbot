"""Streamlit frontend for the Omani AI Therapist."""

import numpy as np
import streamlit as st
import time
import os
from services.session_manager import SessionManager
from utils.audio_utils import AudioRecorder, AudioPlayer
from utils.logging_config import setup_logging
from config import CONFIG, validate_config
import sounddevice as sd

# Setup logging
logger = setup_logging()

# Page configuration
st.set_page_config(
    page_title="Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ø¹Ù…Ø§Ù†ÙŠ - Omani AI Therapist",
    page_icon="ğŸ§ ",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# Custom CSS
st.markdown("""
<style>
.main-header {
    text-align: center;
    color: #2E8B57;
    font-size: 2.5em;
    margin-bottom: 30px;
    font-weight: bold;
}

.status-box {
    color: #031a2e;
    padding: 20px;
    border-radius: 10px;
    margin: 20px 0;
    text-align: center;
}

.status-listening {
    border: 2px solid #4CAF50;
}

.status-processing {
    border: 2px solid #FF9800;
}

.status-ready {
    border: 2px solid #2196F3;
}

.response-box {
    background-color: #F5F5F5;
    padding: 20px;
    border-radius: 10px;
    margin: 20px 0;
    border-left: 5px solid #2E8B57;
}

.emotion-indicator {
    display: inline-block;
    padding: 5px 15px;
    border-radius: 20px;
    margin: 5px;
    font-weight: bold;
}

.emotion-positive {
    background-color: #4CAF50;
    color: white;
}

.emotion-negative {
    background-color: #F44336;
    color: white;
}

.emotion-neutral {
    background-color: #9E9E9E;
    color: white;
}

.sidebar-info {
    background-color: #031a2e;
    padding: 15px;
    border-radius: 10px;
    margin: 10px 0;
}

.arabic-text {
    font-family: 'Tahoma', 'Arial Unicode MS', sans-serif;
    font-size: 1.2em;
    line-height: 1.6;
    direction: rtl;
}
</style>
""", unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state variables."""
    if 'session_manager' not in st.session_state:
        st.session_state.session_manager = SessionManager()
    
    if 'audio_recorder' not in st.session_state:
        st.session_state.audio_recorder = AudioRecorder()
    
    if 'audio_player' not in st.session_state:
        st.session_state.audio_player = AudioPlayer()
    
    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
    
    if 'current_status' not in st.session_state:
        st.session_state.current_status = "ready"
    
    if 'processing_time' not in st.session_state:
        st.session_state.processing_time = 0

    if 'stop_signal' not in st.session_state:
        st.session_state.stop_signal = False

def display_header():
    """Display the main header."""
    st.markdown("""
    <div class="main-header">
        ğŸ§  Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ø¹Ù…Ø§Ù†ÙŠ<br>
        <small>Omani AI Therapist</small>
    </div>
    """, unsafe_allow_html=True)

def display_status(status: str, message: str = ""):
    """Display current status."""
    status_classes = {
        "ready": "status-ready",
        "listening": "status-listening",
        "processing": "status-processing",
        "speaking": "status-speaking"
    }
    
    status_messages = {
        "ready": "ğŸ¤ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªÙ…Ø§Ø¹ - Ready to Listen",
        "listening": "ğŸ‘‚ Ø£Ø³ØªÙ…Ø¹ Ø¥Ù„ÙŠÙƒ - Listening...",
        "processing": "ğŸ¤” Ø£ÙÙƒØ± ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ - Processing your response...",
        "speaking": "ğŸ”Š Ø£Ø¬ÙŠØ¨Ùƒ Ø§Ù„Ø¢Ù† - Speaking..."
    }
    
    display_message = message or status_messages.get(status, "")
    css_class = status_classes.get(status, "status-ready")
    
    st.markdown(f"""
    <div class="status-box {css_class}">
        <h3>{display_message}</h3>
    </div>
    """, unsafe_allow_html=True)

def display_emotions(emotions: dict):
    """Display emotion indicators."""
    if not emotions:
        return
    
    st.markdown("### Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ÙƒØªØ´ÙØ© - Detected Emotions")
    
    emotion_html = ""
    for emotion, confidence in emotions.items():
        if confidence > 0.1:  # Only show emotions with significant confidence
            emotion_class = f"emotion-{emotion}"
            emotion_html += f"""
            <span class="{emotion_class} emotion-indicator">
                {emotion}: {confidence:.1%}
            </span>
            """
    
    if emotion_html:
        st.markdown(emotion_html, unsafe_allow_html=True)

def display_sidebar():
    """Display sidebar with app information and controls."""
    with st.sidebar:
        st.markdown("## Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ - App Info")
        
        st.markdown("""
        <div class="sidebar-info">
            <h3>ğŸ¯ Ø§Ù„Ù‡Ø¯Ù - Purpose</h3>
            <p>Ù…Ø³Ø§Ø¹Ø¯ Ù†ÙØ³ÙŠ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ù…Ø§Ù†ÙŠØ©</p>
            <p>AI therapist speaking in Omani dialect</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="sidebar-info">
            <h3>ğŸ”§ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª - Technologies</h3>
            <ul>
                <li>ğŸ¤ OpenAI Whisper (STT)</li>
                <li>ğŸ§  GPT-4 + Claude Opus</li>
                <li>ğŸ’­ Arabic Emotion Detection</li>
                <li>ğŸ”Š Coqui XTTS v2 (TTS)</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="sidebar-info">
            <h3>ğŸ“ Ø·ÙˆØ§Ø±Ø¦ - Emergency</h3>
            <p><strong>Ø®Ø· Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù†ÙØ³ÙŠØ©:</strong><br>
            ğŸ†˜ 16262</p>
            <p><strong>Mental Health Helpline:</strong><br>
            ğŸ†˜ 16262</p>
        </div>
        """, unsafe_allow_html=True)
        
        if st.session_state.processing_time > 0:
            st.markdown(f"""
            <div class="sidebar-info">
                <h3>â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© - Processing Time</h3>
                <p><strong>{st.session_state.processing_time:.2f} seconds</strong></p>
            </div>
            """, unsafe_allow_html=True)
        
        show_debug = st.checkbox("Ø¥Ø¸Ù‡Ø§Ø± ØªÙØ§ØµÙŠÙ„ ØªÙ‚Ù†ÙŠØ© - Show Debug Info")
        
        if show_debug:
            st.markdown("### Debug Information")
            st.json({
                "OpenAI API": bool(CONFIG.openai_api_key),
                "Anthropic API": bool(CONFIG.anthropic_api_key),
                "Voice File": os.path.exists(CONFIG.voice_file_path),
                "Max Response Time": CONFIG.max_response_time,
                "Sample Rate": CONFIG.sample_rate
            })

def stop_talking():
    """Stops current recording/playback and resets state."""
    logger.info("Stop button clicked. Setting stop_signal.")
    st.session_state.stop_signal = True # Signal to stop
    
    # Try to stop sounddevice playback immediately
    try:
        if sd.get_stream().active: # Check if a sounddevice stream is active
            sd.stop()
            sd.wait() # Wait briefly for it to finish stopping
            logger.info("sounddevice playback stopped.")
    except sd.PortAudioError as e:
        logger.warning(f"Could not stop sounddevice stream (might not be active): {e}")
    except Exception as e:
        logger.error(f"Unexpected error when trying to stop sounddevice: {e}")

    # Reset statuses
    st.session_state.current_status = "ready"
    st.session_state.audio_bytes = None # Clear any pending audio
    # The record_audio function itself needs to check st.session_state.stop_signal
    # and exit gracefully if it's set. This requires a change in audio_utils.py as well.
    st.rerun() # Rerun to update the UI and stop any loops


def main():
    """Main application function."""
    try:
        # Validate configuration
        validate_config()
        
        # Initialize session state
        initialize_session_state()
        
        # Display header
        display_header()
        
        # Display sidebar and get settings
        display_sidebar()
        
        # Create a container for the chat messages
        chat_container = st.container()

        # Display conversation history in the chat container first
        with chat_container:
            for entry in st.session_state.conversation_history:
                with st.chat_message("user"):
                    st.markdown(f'<p class="arabic-text">{entry["user"]}</p>', unsafe_allow_html=True)
                with st.chat_message("assistant"):
                    st.markdown(f'<p class="arabic-text">{entry["therapist"]}</p>', unsafe_allow_html=True)
                    if entry.get('emotions'):
                        # Optionally display emotions here, or in debug only
                        pass # display_emotions(entry['emotions'])
        
        # Display current status
        display_status(st.session_state.current_status)
        
        # Main interaction area
        col1, col2, col3 = st.columns([1, 2, 1])
        
        with col2:
            button_label = "ğŸ¤ Ø§Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© - Start talking"
            button_key = "record_btn"
            button_help = "Ø§Ø¶ØºØ· Ù„Ø¨Ø¯Ø¡ Ø§Ù„ÙƒÙ„Ø§Ù…"
            is_button_disabled = False

            if st.session_state.current_status != "ready":
                button_label = "ğŸš« Ø¥ÙŠÙ‚Ø§Ù - Stop"
                button_key = "stop_btn" # Use a different key to avoid conflicts
                button_help = "Ø§Ø¶ØºØ· Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø£Ùˆ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ùˆ Ø§Ù„ØªØ­Ø¯Ø«"

            # Create the button
            if st.button(button_label, 
                         key=button_key,
                         help=button_help,
                         use_container_width=True,
                         on_click=stop_talking if st.session_state.current_status != "ready" else None): # Call stop_talking if not ready
                
                # If current_status is "ready", it means "Start talking" was clicked
                if st.session_state.current_status == "ready":
                    st.session_state.stop_signal = False # Ensure stop signal is false when starting
                    st.session_state.current_status = "listening"
                    st.rerun()
            
            # Processing area
            if st.session_state.current_status == "listening":
                with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„... Recording..."):
                    try:
                        # Record audio
                        audio_bytes = st.session_state.audio_recorder.record_audio()

                        if st.session_state.stop_signal:
                            logger.info("Recording stopped by user.")
                            st.session_state.stop_signal = False # Reset signal
                            st.session_state.current_status = "ready"
                            st.rerun()
                        elif audio_bytes: 
                            st.session_state.audio_bytes = audio_bytes
                            st.session_state.current_status = "processing"
                            st.rerun()
                        else: # No speech detected
                            st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„ÙƒØ´Ù Ø¹Ù† ÙƒÙ„Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰. - No speech detected. Please try again.")
                            st.session_state.current_status = "ready"
                            st.rerun()
                        
                    except Exception as e:
                        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ - Recording error: {str(e)}")
                        st.session_state.current_status = "ready"
                        st.rerun()
            
            elif st.session_state.current_status == "processing":
                # Display user's transcription immediately after recording
                # This ensures the user's input appears quickly
                if 'user_message_placeholder' not in st.session_state:
                    st.session_state.user_message_placeholder = None

                if st.session_state.user_message_placeholder is None and st.session_state.audio_bytes:
                    with chat_container:
                        with st.chat_message("user"):
                            # Create an empty placeholder to update later with actual transcription
                            st.session_state.user_message_placeholder = st.empty()
                            st.session_state.user_message_placeholder.markdown(f'<p class="arabic-text">...</p>', unsafe_allow_html=True)
                with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©... Processing..."):
                    if st.session_state.stop_signal:
                        logger.info("Processing skipped due to stop signal.")
                        st.session_state.stop_signal = False # Reset signal
                        st.session_state.current_status = "ready"
                        st.rerun()
                        return # Exit the function to prevent further processing
                    try:
                        # Process through pipeline
                        result = st.session_state.session_manager.process_voice_input(st.session_state.audio_bytes, st.session_state.conversation_history)

                        st.session_state.audio_bytes = None  # Clear audio bytes after processing
                        
                        if result["success"]:
                            # --- MODIFIED: Update the user's displayed transcription placeholder ---
                            if st.session_state.user_message_placeholder:
                                st.session_state.user_message_placeholder.markdown(
                                    f'<p class="arabic-text">{result["transcription"]}</p>', 
                                    unsafe_allow_html=True
                                )
                                # Clear the placeholder reference so a new one is created next turn
                                st.session_state.user_message_placeholder = None 

                            # --- ADDED: Display therapist's response in real-time within the chat_container ---
                            with chat_container:
                                with st.chat_message("assistant"):
                                    st.markdown(f'<p class="arabic-text">{result["response_text"]}</p>', unsafe_allow_html=True)
                                    if result.get('emotions'):
                                        display_emotions(result['emotions']) # Display emotions below AI response
                            
                            # Update conversation history
                            st.session_state.conversation_history.append({
                                "user": result["transcription"],
                                "therapist": result["response_text"],
                                "emotions": result["emotions"],
                                "timestamp": time.time()
                            })
                            
                            # Play audio response
                            if result["audio_file"]:
                                st.session_state.audio_bytes = result["audio_file"]
                                st.session_state.current_status = "speaking"
                                st.rerun()
                            else: # If no audio file returned
                                st.session_state.current_status = "ready"
                                st.rerun()
                            
                            # Update processing time
                            st.session_state.processing_time = result["processing_time"]
                            
                            # Check if response was within time limit
                            if result["processing_time"] > CONFIG.max_response_time:
                                st.warning(f"âš ï¸ Ø§Ù„Ø±Ø¯ Ø§Ø³ØªØºØ±Ù‚ {result['processing_time']:.1f} Ø«Ø§Ù†ÙŠØ© (Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨)")

                            
                            # sd.wait()  # Wait until playback is done
                            
                        else:
                            st.error(f"ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© - Processing failed: {result.get('error', 'Unknown error')}")
                        
                        st.session_state.current_status = "listening"
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© - Processing error: {str(e)}")
                        st.session_state.current_status = "ready"
                        st.rerun()
            elif st.session_state.current_status == "speaking":
                with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¯Ø«... Speaking..."):
                    try:
                        # Play the audio response
                        if 'audio_bytes' in st.session_state:
                            audio_bytes = st.session_state.audio_bytes
                            sd.play(np.array(audio_bytes), samplerate=CONFIG.sample_rate)
                            sd.wait()  # Wait until playback is done
                        
                        st.session_state.current_status = "listening"
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ø¯Ø« - Speaking error: {str(e)}")
                        st.session_state.current_status = "ready"
                        st.rerun()
        
        # Footer
        st.markdown("---")
        st.markdown("""
        <div style="text-align: center; color: #666;">
            <p>ğŸ¤– Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ø¹Ù…Ø§Ù†ÙŠ - Omani AI Therapist</p>
            <p>ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨Ø£Ø­Ø¯Ø« ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ - Powered by Advanced AI</p>
        </div>
        """, unsafe_allow_html=True)
    
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ - Application error: {str(e)}")
        logger.error(f"Application error: {str(e)}")

if __name__ == "__main__":
    main()